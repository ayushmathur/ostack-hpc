\subsubsection{Setup generic bare metal instance}

	This section configures OpenStack for bare metal instances according to HPC images and user inputs. Before starting this, it is still assumed that system administrator has installed OpenStack and its services.

	Before instantiating bare metal nodes with HPC environment, we need to do little more configuration. This section configures the network for "HPC as a service", uploads the compute OS images to Glance, creates a flavor for bare metal, and uploads public keys for a ssh session.

	Create a generic network for "HPC as a service" with a name "sharednet1"

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_validation_comment # XFILEX
% ohpc_validation_comment # FILE: deploy_chpc_openstack part 2
% ohpc_command function setup_baremetal() {
\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Get the tenant ID for the services tenant
[ctrlr](*\#*)    SERVICES_TENANT_ID=`openstack project list | grep "|\s*services\s*|" | awk '{print $2}'`
[ctrlr](*\#*) 
[ctrlr](*\#*) #Create the flat network on which you are going to launch instances
[ctrlr](*\#*)     neutron net-list | grep "|\s*sharednet1\s*|"
[ctrlr](*\#*)     net_exists=$?
[ctrlr](*\#*)     if [ "${net_exists}" -ne "0" ]; then
[ctrlr](*\#*)         neutron net-create --tenant-id ${SERVICES_TENANT_ID} sharednet1 --shared \
[ctrlr](*\#*)          --provider:network_type flat --provider:physical_network physnet1
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     NEUTRON_NETWORK_UUID=`neutron net-list | grep "|\s*sharednet1\s*|" | awk '{print $2}'`
\end{lstlisting}
% end_ohpc_run

	Setup node cleaning network and restart ironic conduction. This became mendatory in ocata release of OpenStack

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*)     if [ "${nova_version_major}" == 7 ]; then
[ctrlr](*\#*)         if ! grep -q "^cleaning_network" /etc/ironic/ironic.conf; then
[ctrlr](*\#*)            sed --in-place "s|^#cleaning_network.*|cleaning_network = \
[ctrlr](*\#*)                  ${NEUTRON_NETWORK_UUID}|" /etc/ironic/ironic.conf
[ctrlr](*\#*)         else
[ctrlr](*\#*)            sed --in-place "s|^cleaning_network.*|cleaning_network = \
[ctrlr](*\#*)                  ${NEUTRON_NETWORK_UUID}|" /etc/ironic/ironic.conf
[ctrlr](*\#*)         fi
[ctrlr](*\#*)         #restart the ironic conductor 
[ctrlr](*\#*)         systemctl restart openstack-ironic-conductor
[ctrlr](*\#*)     fi
\end{lstlisting} 
% end_ohpc_run

	Create a subnet for our cluster with user defined start and end IP addresses. Mark the controller as a gateway for our instances.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Create the subnet on the newly created network
[ctrlr](*\#*)     neutron subnet-list | grep "|\s*subnet01\s*|"
[ctrlr](*\#*)     subnet_exists=$?
[ctrlr](*\#*)     if [ "${subnet_exists}" -ne "0" ]; then
[ctrlr](*\#*)         neutron subnet-create sharednet1 --name subnet01 --ip-version=4 --gateway=${controller_ip} \
[ctrlr](*\#*)          --allocation-pool start=${cc_subnet_dhcp_start},end=${cc_subnet_dhcp_end} \
[ctrlr](*\#*)          --enable-dhcp ${cc_subnet_cidr}
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     NEUTRON_SUBNET_UUID=`neutron subnet-list | grep "|\s*subnet01\s*|" | awk '{print $2}'`
\end{lstlisting} 
% end_ohpc_run

	Upload kernel and initrd images to Glance, so that they are available to Ironic while deploying the "nodes" (including both the SMS and the compute nodes).

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Create the deploy-kernel and deploy-initrd images
[ctrlr](*\#*)     glance image-list | grep "|\s*deploy-vmlinuz\s*|"
[ctrlr](*\#*)     img_exists=$?
[ctrlr](*\#*)     if [ "${img_exists}" -ne "0" ]; then
[ctrlr](*\#*)         glance image-create --name deploy-vmlinuz --visibility public --disk-format \
[ctrlr](*\#*)          aki --container-format aki < ${chpc_image_deploy_kernel}
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     DEPLOY_VMLINUZ_UUID=`glance image-list | grep "|\s*deploy-vmlinuz\s*|" | awk '{print $2}'`
[ctrlr](*\#*) 
[ctrlr](*\#*)     glance image-list | grep "|\s*deploy-initrd\s*|"
[ctrlr](*\#*)     img_exists=$?
[ctrlr](*\#*)     if [ "${img_exists}" -ne "0" ]; then
[ctrlr](*\#*)         glance image-create --name deploy-initrd --visibility public --disk-format ari \
[ctrlr](*\#*)         --container-format ari < ${chpc_image_deploy_ramdisk}
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     DEPLOY_INITRD_UUID=`glance image-list | grep "|\s*deploy-initrd\s*|" | awk '{print $2}'`
\end{lstlisting} 
% end_ohpc_run

	Create a bare metal flavor with Nova.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Create the baremetal flavor and set the architecture to x86_64
[ctrlr](*\#*) # This will create common baremetal flavor, if SMS node & compute has different
[ctrlr](*\#*) # characteristic than user shall create multiple flavor one for each characterisitc
[ctrlr](*\#*)     nova flavor-list | grep "|\s*baremetal-flavor\s*|"
[ctrlr](*\#*)     flavor_exists=$?
[ctrlr](*\#*)     if [ "$flavor_exists" -ne "0" ]; then
[ctrlr](*\#*)         nova flavor-create baremetal-flavor baremetal-flavor ${RAM_MB} ${DISK_GB} ${CPU}
[ctrlr](*\#*)         nova flavor-key baremetal-flavor set cpu_arch=$ARCH
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     FLAVOR_UUID=`nova flavor-list | grep "|\s*baremetal-flavor\s*|" | awk '{print $2}'`
[ctrlr](*\#*) #Increase the Quota limit for admin to allow nova boot
[ctrlr](*\#*)     openstack quota set --ram 512000 --cores 1000 --instances 100 admin
\end{lstlisting} 
% end_ohpc_run

	Register your public ssh keys with Nova, so that admin can ssh to the node without an interactive password.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Register SSH keys with Nova
[ctrlr](*\#*)  nova keypair-list | grep "|\s*ostack_key\s*|"
[ctrlr](*\#*)  keypair_exists=$?
[ctrlr](*\#*)  if [ "${keypair_exists}" -ne "0" ]; then
[ctrlr](*\#*)     nova keypair-add --pub-key ${HOME}/.ssh/id_rsa.pub ostack_key
[ctrlr](*\#*)  fi
\end{lstlisting} 
% end_ohpc_run

	Export keypair name for use it later in other sections.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*)     KEYPAIR_NAME=ostack_key
\end{lstlisting} 
% ohpc_command }
% end_ohpc_run
\newpage
\subsubsection{Setup HPC head node}


	In the previous section, we created a generic bare metal setup. In this section we will create a configuration for the HPC head node in an OpenStack cloud.
	
	We created HPC head node OS image in previous sections, so let's upload these images to Glance, and store IMAGE id in environment variable 'SMS\_DISK\_IMAGE\_UUID', to be used during boot. 

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command function setup_sms() {
\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # Create sms node image
[ctrlr](*\#*)    glance image-list | grep "|\s*sms-image\s*|"
[ctrlr](*\#*)    img_exists=$?
[ctrlr](*\#*)    if [ "${img_exists}" -ne "0" ]; then
[ctrlr](*\#*)        glance image-create --name sms-image --visibility public --disk-format qcow2 \
[ctrlr](*\#*)          --container-format bare < ${chpc_image_sms}
[ctrlr](*\#*)    fi
[ctrlr](*\#*)    SMS_DISK_IMAGE_UUID=`glance image-list | grep "|\s*sms-image\s*|" | awk '{print $2}'`
\end{lstlisting} 
% end_ohpc_run

	For provisioning the sms node with Ironic, we need to register the node with Ironic. This is done by registering the node's BMC, and characteristic (the OpenStack flavor) like memory, cpu, disk space and architecture, and also registering kernel boot images. We will use the pxe\_ipmitool as a provisioning driver in Ironic with a boot mode as bios.

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Create a sms node in the bare metal service Ironic.
[ctrlr](*\#*)     ironic node-list | grep "|\s*${sms_name}$\s*|"
[ctrlr](*\#*)     node_exists=$?
[ctrlr](*\#*)     if [ "${node_exists}" -ne "0" ]; then 
[ctrlr](*\#*)         ironic node-create -d pxe_ipmitool -i deploy_kernel=${DEPLOY_VMLINUZ_UUID} -i \
[ctrlr](*\#*)         deploy_ramdisk=${DEPLOY_INITRD_UUID} -i ipmi_terminal_port=8023 -i ipmi_address=${sms_bmc} \
[ctrlr](*\#*)         -i ipmi_username=${sms_bmc_username} -i ipmi_password=${sms_bmc_password} -p cpus=${CPU} -p \
[ctrlr](*\#*)         memory_mb=${RAM_MB} -p local_gb=${DISK_GB} -p cpu_arch=${ARCH} -p \
[ctrlr](*\#*)          capabilities="boot_mode:bios" -n ${sms_name}
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     SMS_UUID=`ironic node-list | grep "|\s*${sms_name}\s*|" | awk '{print $2}'`
\end{lstlisting} 
% end_ohpc_run

	Now, we need tell Ironic about the network port, on which the node will perform pxe boot by configuring the MAC address(es). 


% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Add the associated port(s) MAC address to the created node(s)
[ctrlr](*\#*)     ironic port-create -n ${SMS_UUID} -a ${sms_mac}
\end{lstlisting} 
% end_ohpc_run

Add the instance info and disk space for root. \newline
	Add the instance\_info/image\_source and instance\_info/root\_gb.
    
% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*)     ironic node-update $SMS_UUID add \
[ctrlr](*\#*)          instance_info/image_source=${SMS_DISK_IMAGE_UUID} instance_info/root_gb=50
\end{lstlisting} 
% end_ohpc_run


	We will assign a fixed IP address to the SMS node. This is done by associating the SMS node MAC address with its neutron port. We will store this information in neutron with sms\_name. We will also set the environment variable, SMS\_PORT\_ID, with this port id, to be used during boot.

% begin_ohpc_run
% ohpc_validation_newline
    
    \begin{lstlisting}[language=bash,keywords={}]
    [ctrlr](*\#*) #Setup neutron port for static IP addressing of sms node.
    [ctrlr](*\#*) neutron port-create sharednet1 --dns_name $sms_name --fixed-ip \
    [ctrlr](*\#*)          ip_address=$sms_ip --name $sms_name --mac-address $sms_mac
    [ctrlr](*\#*)     SMS_PORT_ID=`neutron port-list | grep "|\s*$sms_name\s*|" | awk '{print $2}'`
	\end{lstlisting} 
% ohpc_command }
% end_ohpc_run

\newpage
\subsubsection{Setup HPC compute nodes}

	In a previous section we configured OpenStack to instantiate the SMS node. In this section we will be configuring OpenStack to instantiate the HPC compute nodes.

	For HPC compute nodes, we created compute node images, uploaded those HPC compute node images to glance as a user image, and stored IMAGE id in the environment variable USER\_DISK\_IMAGE\_UUID, to be used during boot.

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command function setup_cn() {

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Create the whole-disk-image from the user's qcow2 file
[ctrlr](*\#*)     glance image-list | grep "|\s*user-image\s*|"
[ctrlr](*\#*)     img_exists=$?
[ctrlr](*\#*)     if [ "${img_exists}" -ne "0" ]; then
[ctrlr](*\#*)         glance image-create --name user-image --visibility public --disk-format \
[ctrlr](*\#*)          qcow2 --container-format bare < ${chpc_image_user}
[ctrlr](*\#*)     fi
[ctrlr](*\#*)     USER_DISK_IMAGE_UUID=`glance image-list | grep "|\s*user-image\s*|" | awk '{print $2}'`
\end{lstlisting} 
% end_ohpc_run


	Similar to the SMS node, we will create the setup for all compute nodes, including creating the Ironic node, associating node MAC addresses, adding instance information, and assigning fixed IP addresses. In our example we used 4 HPC compute nodes. To store the information in each OpenStack component, we will assign compute node host name as a name, which is host name prefix (as chosen by user in inputs), followed by a node counter. 

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # Setup Compute nodes
[ctrlr](*\#*) # Note: if installed from the rpm, the following script is installed as setup_compute_nodes.sh 
[ctrlr](*\#*) 
[ctrlr](*\#*)     for ((i=0; i < ${num_ccomputes}; i++)); do
[ctrlr](*\#*)         #Create compute nodes in the bare metal service
[ctrlr](*\#*)         ironic node-list | grep "|\s*${cnodename_prefix}$((i+1))\s*|"
[ctrlr](*\#*)         node_exists=$?
[ctrlr](*\#*)         if [ "${node_exists}" -ne "0" ]; then
[ctrlr](*\#*)             ironic node-create -d pxe_ipmitool -i \
[ctrlr](*\#*)          deploy_kernel=${DEPLOY_VMLINUZ_UUID} -i deploy_ramdisk=${DEPLOY_INITRD_UUID} \
[ctrlr](*\#*)         -i ipmi_terminal_port=8023 -i ipmi_address=${cc_bmc[$i]} \
[ctrlr](*\#*)         -i ipmi_username=${cc_bmc_username} -i ipmi_password=${cc_bmc_password} -p cpus=${CPU} \
[ctrlr](*\#*)          -p memory_mb=${RAM_MB} -p local_gb=${DISK_GB} -p cpu_arch=${ARCH} -p \
[ctrlr](*\#*)          capabilities="boot_mode:bios" -n ${cnodename_prefix}$((i+1))
[ctrlr](*\#*)        fi
[ctrlr](*\#*)        NODE_UUID_CC[$i]=`ironic node-list | grep \
[ctrlr](*\#*)          "|\s*${cnodename_prefix}$((i+1))\s*|" | awk '{print $2}'`

[ctrlr](*\#*)         # update for compute nodes node MAC
[ctrlr](*\#*)         ironic port-create -n ${NODE_UUID_CC[$i]} -a ${cc_mac[$i]}

[ctrlr](*\#*)         #Add the instance_info/image_source and instance_info/root_gb
[ctrlr](*\#*)         ironic node-update ${NODE_UUID_CC[$i]} add \
[ctrlr](*\#*)          instance_info/image_source=${USER_DISK_IMAGE_UUID} instance_info/root_gb=50
[ctrlr](*\#*) 
[ctrlr](*\#*)         #Setup neutron port for static IP addressing of compute nodes
[ctrlr](*\#*)         cn_name=${cnodename_prefix}$((i+1))
[ctrlr](*\#*)         neutron port-create sharednet1 --dns_name $cn_name --fixed-ip \
[ctrlr](*\#*)          ip_address=${cc_ip[$i]} --name $cn_name --mac-address ${cc_mac[$i]}
[ctrlr](*\#*)         NEUTRON_PORT_ID_CC[$i]=`neutron port-list | grep \
[ctrlr](*\#*)          "|\s*${cnodename_prefix}$((i+1))\s*|" | awk '{print $2}'`
[ctrlr](*\#*)     done
\end{lstlisting} 
% end_ohpc_run
\newpage

	Ironic periodically syncs with Nova for available nodes. Nova then updates its records for all available hosts. So, before booting the node with Nova, allow some time to sync Ironic with it. 

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) # Wait for the Nova hypervisor-stats to sync with available Ironic resources
[ctrlr](*\#*) sleep 121
\end{lstlisting} 
% end_ohpc_run

	For OpenStack ocata release, we need to configure cell with host discovery using nova-manage 

% begin_ohpc_run
% ohpc_validation_newline
\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*)     # configure cell in nova 
[ctrlr](*\#*)     nova-manage cell_v2 discover_hosts
\end{lstlisting} 
% ohpc_command }
% end_ohpc_run

\subsubsection{Boot SMS node}

	In another previous section, we completed the bare metal configuration. The user can request any available baremetal nodes by specifying the flavor he wants and image to use to boot the node. For bare metal, we created a flavor with name 'baremetal-flavor', we will provide this to Nova with a CLI option --flavor. In our situation we will request 1 bare metal node with a baremetal flavor (--flavor) and an SMS node image (--image) to boot.  We also would like to reserve the IP address of this node. 

	In a previous section (setup sms), we associated one of the node's MAC address with its IP address, we will request this from Nova by indicating the port-id we created earlier (port-id=\${SMS\_PORT\_ID}). 

	Also in a previous section, we created a cloud-init script for the SMS node. We will provide the cloud-init script (chpcSMSInit) to the Nova CLI option --user-data. For cloud init we will use a metadata server, which will be provided by "--meta role= option". We will then name the SMS node with the host name of a booted bare metal node. 

	Before booting, save boot command to a script, which can be used to re-instantiate the same node at a later time."

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command function boot_sms() {

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) #Boot the SMS node with nova. chpcInit is set from prepare_cloudInit
[ctrlr](*\#*) echo "nova boot --config-drive true --flavor ${FLAVOR_UUID} --image \
[ctrlr](*\#*)          ${SMS_DISK_IMAGE_UUID} --key-name ${KEYPAIR_NAME} --meta role=webservers \
[ctrlr](*\#*)          --user-data=$chpcSMSInit --nic port-id=${SMS_PORT_ID} ${sms_name}" > boot_sms
\end{lstlisting} 
% end_ohpc_run

	Issue a boot command to Nova to boot the SMS node:

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) nova boot --config-drive true --flavor ${FLAVOR_UUID} --image ${SMS_DISK_IMAGE_UUID} \
[ctrlr](*\#*)          --key-name ${KEYPAIR_NAME} --meta role=webservers --user-data=$chpcSMSInit \
[ctrlr](*\#*)         --nic port-id=${SMS_PORT_ID} ${sms_name}
\end{lstlisting} 
% end_ohpc_run

	Wait 15 seconds before booting the compute nodes. This will allow enough time to boot the SMS node before the compute nodes start. 

% begin_ohpc_run
% ohpc_validation_newline

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) sleep 15
\end{lstlisting} 
% ohpc_command }
% end_ohpc_run

\newpage
\subsubsection{Boot compute nodes}

	Booting compute nodes is very similar to booting SMS nodes. In our case we will boot 4 compute nodes (as specified in user inputs). The host name of each compute node will use prefix defined by the 'cnodename\_prefix' variable, followed by node counter. For compute nodes, we will use a compute node image (USER\_DISK\_IMAGE\_UUID) and compute node cloud-init script (chpcInit). 

% begin_ohpc_run
% ohpc_validation_newline
% ohpc_command function boot_cn() {

\begin{lstlisting}[language=bash,keywords={}]
[ctrlr](*\#*) for ((i=0; i < ${num_ccomputes}; i++)); do
[ctrlr](*\#*) filename="cn$((i+1))"
[ctrlr](*\#*) echo "nova boot --config-drive true --flavor ${FLAVOR_UUID} --image \
[ctrlr](*\#*)          ${USER_DISK_IMAGE_UUID} --key-name ${KEYPAIR_NAME} --meta role=webservers \
[ctrlr](*\#*)          --user-data=$chpcInit --nic port-id=${NEUTRON_PORT_ID_CC[$i]} \
[ctrlr](*\#*)          ${cnodename_prefix}$((i+1))" > boot_$filename
[ctrlr](*\#*) nova boot --config-drive true --flavor ${FLAVOR_UUID} --image \
[ctrlr](*\#*)         ${USER_DISK_IMAGE_UUID} --key-name ${KEYPAIR_NAME} --meta role=webservers \
[ctrlr](*\#*)          --user-data=$chpcInit --nic port-id=${NEUTRON_PORT_ID_CC[$i]} \
[ctrlr](*\#*)          ${cnodename_prefix}$((i+1))
[ctrlr](*\#*) #wait for 5 seconds before booting other compute node
[ctrlr](*\#*) sleep 5
[ctrlr](*\#*) done
\end{lstlisting}
% ohpc_command }
% ohpc_validation_comment ## Initial setup baremetal environment 
% ohpc_command setup_baremetal
% ohpc_validation_comment ## Setup SMS node first
% ohpc_command setup_sms
% ohpc_validation_comment ## Setup compute nodes 
% ohpc_command setup_cn
% ohpc_validation_comment ## Wait for the Nova hypervisor-stats to sync with available Ironic resources
% ohpc_command #sleep 121
% ohpc_validation_comment ## Boot sms node
% ohpc_command boot_sms
% ohpc_validation_comment ## # wait for 15 seconds before starting to boot compute nodes. TBD need to tune this time
% ohpc_validation_comment ## # SMS node should be booted before compute nodes starts booting. At minimum
% ohpc_validation_comment ## # sms node shall have cloud init executed before CN's cloud init
% ohpc_command #sleep 15
% ohpc_validation_comment ## # Now boot compute nodes
% ohpc_command boot_cn
% ohpc_command #sleep 5

% end_ohpc_run
